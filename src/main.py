import re
import os
import time
import json
import pandas as pd
from pathlib import Path

import get_data_v3 
from api_genai import GEMINI_CLIENT, RPM_DELAY_SECONDS, classify_payments_batch 

# ----------------------------------------------------------------------
# 1. í™˜ê²½ ì„¤ì • ë° ìƒìˆ˜ ì •ì˜
# ----------------------------------------------------------------------
PROJECT_ROOT = Path(__file__).resolve().parent.parent if '__file__' in locals() else Path('.')
LOG_DIR_PATH = PROJECT_ROOT / 'log'
LOG_DIR_PATH.mkdir(exist_ok=True)

# AI ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
MAX_BATCH_SIZE = 20
# get_data_v3ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì›ë³¸ ì»¬ëŸ¼ ëª©ë¡
FINAL_COLUMNS = ['ê±°ë˜ì¼ì‹œ', 'ì‚¬ìš©êµ¬ë¶„', 'ì‚¬ìš©ë‚´ì—­', 'ê±°ë˜ìƒëŒ€ë°©', 'ì…ê¸ˆì•¡', 'ì¶œê¸ˆì•¡', 'ì”ì•¡', 'ë©”ëª¨', 'ì¶œì²˜']
# AIê°€ ë¦¬í„´í•˜ëŠ” ê²°ê³¼ ì»¬ëŸ¼ ëª©ë¡
AI_RESULT_COLUMNS = ['ì¸í’‹_ë¬¸ì¥', 'ê±°ë˜_ìœ í˜•', 'ì£¼ìš”_ì¹´í…Œê³ ë¦¬', 'ì„¸ë¶€_ì¹´í…Œê³ ë¦¬', 'íŒë‹¨_ì‚¬ìœ ']


def run_full_pipeline():
    """
    ë°ì´í„° í†µí•©, AI ë¶„ë¥˜(ì¸ë±ìŠ¤ ê¸°ë°˜), ê²°ê³¼ ë³‘í•© ë° ìµœì¢… TSV ì €ì¥ì„ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜í•©ë‹ˆë‹¤.
    """
    
    # 1ë‹¨ê³„: ì›ë³¸ ë°ì´í„° í†µí•© ë° ì¤€ë¹„ (get_data_v3.py í˜¸ì¶œ)
    print("1ë‹¨ê³„: ì›ë³¸ ë°ì´í„° í†µí•© ë° ì „ì²˜ë¦¬ (get_data_v3.py í˜¸ì¶œ)")
    # df_originalì€ reset_index(drop=True)ë¥¼ ê±°ì³ 0ë¶€í„° ì‹œì‘í•˜ëŠ” ê¹¨ë—í•œ ì¸ë±ìŠ¤ë¥¼ ê°€ì§
    df_original = get_data_v3.run_data_integration_pipeline()

    if df_original is None or df_original.empty:
        print("âŒ í†µí•© ë°ì´í„°ê°€ ì—†ì–´ íŒŒì´í”„ë¼ì¸ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # AI ì…ë ¥ ë°ì´í„° ì¤€ë¹„: TSV ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
    data_length = len(df_original)
    
    # AI ì…ë ¥ ë°ì´í„°ëŠ” TSV ë¬¸ìì—´ í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    # ì¸ë±ìŠ¤ ëŒ€ì‹  ìˆœìˆ˜ ë°ì´í„°ë§Œ í¬í•¨í•˜ì—¬ AIì— ì „ë‹¬í•©ë‹ˆë‹¤.
    data_for_ai = df_original[FINAL_COLUMNS].to_csv(
        sep='\t', header=False, index=False
    ).strip().split('\n')
    
    # 2ë‹¨ê³„: AI ë°°ì¹˜ ì²˜ë¦¬ ë° ê²°ê³¼ ìˆ˜ì§‘ (api_genai.py í˜¸ì¶œ)
    print(f"2ë‹¨ê³„: AI ë°°ì¹˜ ë¶„ë¥˜ ì‹œì‘ (ì¸ë±ìŠ¤ ê¸°ë°˜ ë³‘í•©, ì´ {data_length}ê±´)")

    df_ai_results_list = [] # ê° ë°°ì¹˜ ê²°ê³¼ë¥¼ ë‹´ì„ DataFrame ë¦¬ìŠ¤íŠ¸
    
    for i in range(0, data_length, MAX_BATCH_SIZE):
        batch = data_for_ai[i:i + MAX_BATCH_SIZE]
        batch_num = i // MAX_BATCH_SIZE + 1
        total_batches = int((data_length + MAX_BATCH_SIZE - 1) / MAX_BATCH_SIZE)
        
        print(f"\n--- [ë°°ì¹˜ {batch_num} / ì´ {total_batches} ({len(batch)}ê±´ ì²˜ë¦¬)] ---")
        
        if GEMINI_CLIENT:
            result_json_string = classify_payments_batch(batch, GEMINI_CLIENT)
            time.sleep(2.0) 
        else:
            print("âŒ AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨. ë¶„ë¥˜ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            break

        # JSON íŒŒì‹± ë° DataFrame ìƒì„±
        try:
            # AI ì‘ë‹µ íŒŒì‹± ì•ˆì •í™”: ì •ê·œì‹ì„ ì‚¬ìš©í•´ ìœ íš¨í•œ JSON ë°°ì—´ '[...]'ë§Œ ì¶”ì¶œ
            match = re.search(r'\[.*\]', result_json_string, re.DOTALL)
            
            if not match:
                print(f"âŒ ë°°ì¹˜ {batch_num} JSON ë°°ì—´([...] í¬ë§·)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ ì‘ë‹µ: {result_json_string[:100]}...")
                time.sleep(RPM_DELAY_SECONDS)
                continue
                
            cleaned_json_string = match.group(0)
            parsed_json_list = json.loads(cleaned_json_string)
            
            # í•µì‹¬: JSON ë¦¬ìŠ¤íŠ¸ë¥¼ ë°”ë¡œ DataFrameìœ¼ë¡œ ë³€í™˜
            df_batch_result = pd.DataFrame(parsed_json_list)
            
            # ì…ë ¥ ë°ì´í„° ìˆœì„œì™€ AI ê²°ê³¼ ìˆœì„œê°€ ë™ì¼í•˜ë¯€ë¡œ, ì¸ë±ìŠ¤ë¥¼ ì¬ì •ì˜í•˜ê³  ì¶”ê°€
            # ì¸ë±ìŠ¤ë¥¼ 0ë¶€í„° ì‹œì‘í•˜ë„ë¡ ì¬ì„¤ì •
            df_batch_result = df_batch_result.reset_index(drop=True) 

            # AI ê²°ê³¼ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ê³  ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            if not df_batch_result.empty:
                 # í•„ìš”í•œ AI ì»¬ëŸ¼ë§Œ ì„ íƒ (ì¸í’‹_ë¬¸ì¥ í¬í•¨)
                selected_columns = [col for col in AI_RESULT_COLUMNS if col in df_batch_result.columns]
                df_ai_results_list.append(df_batch_result[selected_columns])
                print(f"âœ… ë°°ì¹˜ {batch_num} ê²°ê³¼ {len(df_batch_result)}ê±´ í†µí•© ì™„ë£Œ.")

        except json.JSONDecodeError as e:
            print(f"âŒ ë°°ì¹˜ {batch_num} JSON íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ. ì›ì¸: {e}. ì›ë³¸ ì‘ë‹µ í™•ì¸ í•„ìš”.")
            time.sleep(RPM_DELAY_SECONDS)
            continue
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            time.sleep(RPM_DELAY_SECONDS)
            continue
            
        time.sleep(RPM_DELAY_SECONDS) # API í˜¸ì¶œ ê°„ ì§€ì—° ì‹œê°„

    
    # 3ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ë³‘í•© ë° íŒŒì¼ ì €ì¥
    print("3ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ë³‘í•© ë° TSV íŒŒì¼ ì €ì¥")

    if not df_ai_results_list:
        print("âŒ AI ë¶„ë¥˜ ê²°ê³¼ê°€ ì—†ì–´ ìµœì¢… íŒŒì¼ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 3.1. ìµœì¢… AI ê²°ê³¼ DataFrame ìƒì„±
    # ignore_index=Trueë¡œ ëª¨ë“  ë°°ì¹˜ ê²°ê³¼ë¥¼ 0ë¶€í„° ì‹œì‘í•˜ëŠ” ì—°ì† ì¸ë±ìŠ¤ë¡œ í•©ì¹¨
    df_ai_final = pd.concat(df_ai_results_list, ignore_index=True)

    # 3.2. ì›ë³¸ DataFrameê³¼ AI ê²°ê³¼ DataFrame ë³‘í•©
    # í•µì‹¬: ì¸ë±ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ íš¡ë°©í–¥(axis=1) ë³‘í•©. ìˆœì„œê°€ ë³´ì¥ë˜ì–´ì•¼ í•¨.
    # ë³‘í•© ì „ì— df_originalì´ df_ai_finalê³¼ í–‰ ê°œìˆ˜ê°€ ê°™ì•„ì•¼ í•¨.
    if len(df_original) != len(df_ai_final):
        print(f"âš ï¸ ê²½ê³ : ì›ë³¸ ë°ì´í„°({len(df_original)}ê±´)ì™€ AI ê²°ê³¼({len(df_ai_final)}ê±´)ì˜ ê°œìˆ˜ê°€ ë¶ˆì¼ì¹˜í•©ë‹ˆë‹¤. TSV íŒŒì¼ì—ëŠ” ë³‘í•©ëœ ê²°ê³¼({len(df_ai_final)}ê±´)ë§Œ í¬í•¨ë©ë‹ˆë‹¤.")
    
    # AI ê²°ê³¼ì˜ 'ì¸í’‹_ë¬¸ì¥'ì€ ìµœì¢… íŒŒì¼ì— ë¶ˆí•„ìš”í•˜ë¯€ë¡œ ì œì™¸
    df_ai_final_clean = df_ai_final.drop(columns=['ì¸í’‹_ë¬¸ì¥'], errors='ignore')
    
    # ì›ë³¸ DataFrameì˜ ì¸ë±ìŠ¤ë¥¼ 'ìˆœë²ˆ' ì»¬ëŸ¼ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ìµœì¢… TSVì— í¬í•¨
    df_original['ìˆœë²ˆ'] = df_original.index + 1
    
    # ì¸ë±ìŠ¤ ê¸°ë°˜ ë³‘í•©
    # concat(axis=1)ì€ ê°™ì€ ì¸ë±ìŠ¤(0, 1, 2, ...)ë¥¼ ê°€ì§„ ë‘ DataFrameì„ ì˜†ìœ¼ë¡œ ë¶™ì—¬ì¤ë‹ˆë‹¤.
    df_final_merged = pd.concat([df_original, df_ai_final_clean], axis=1)

    # 3.3. ìµœì¢… TSV íŒŒì¼ ì €ì¥
    timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
    tsv_file_name = f'final_integrated_classified_{timestamp}.tsv'
    tsv_output_path = LOG_DIR_PATH / tsv_file_name 

    try:
        # ìµœì¢… ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ì˜: ìˆœë²ˆ + ì›ë³¸ ì»¬ëŸ¼ + AI ì»¬ëŸ¼
        # AI_COLUMNSëŠ” 'ê±°ë˜_ìœ í˜•', 'ì£¼ìš”_ì¹´í…Œê³ ë¦¬', 'ì„¸ë¶€_ì¹´í…Œê³ ë¦¬', 'íŒë‹¨_ì‚¬ìœ '
        AI_COLUMNS_ONLY = [c for c in df_ai_final_clean.columns if c != 'ì¸í’‹_ë¬¸ì¥']
        final_columns_order = ['ìˆœë²ˆ'] + FINAL_COLUMNS + AI_COLUMNS_ONLY
        
        df_final_merged[final_columns_order].to_csv(tsv_output_path, sep='\t', index=False, encoding='utf-8')
        print(f"**ğŸŒŸ ìµœì¢… TSV íŒŒì¼ ì €ì¥ ì„±ê³µ:** {tsv_output_path} ({len(df_final_merged)}ê±´)")
    except Exception as e:
        print(f"âŒ ìµœì¢… TSV íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    run_full_pipeline()